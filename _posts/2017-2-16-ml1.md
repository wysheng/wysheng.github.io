---
layout: post
title:  "机器学习笔记：KNN算法"
date:   2017-2-16 15:45:28
categories: 机器学习
tags:  机器学习 python
---
* content
{:toc}
## k-近邻算法
优点：精度高、对异常值不敏感、无数据输入假定。
缺点：计算复杂度高、空间复杂度高
适用数据范围：数值型和标称型
k的含义：我们只选择样本数据集中前k个最相似的数据，通常不大于20，在这k个数据集中选择出现次数最多的分类作为新数据的分类。
kNN算法有很多不同类型，这里介绍的是利用字典存储每个标签出现的频率，operator操作键值进行排序，返回出现最多的分类名称的方式。

## python3.6 的k-近邻算法解读 
```
#kNN算法
def classify0(inX,dataSet,labels,k):                 #inX是用于分类的向量，也即测试向量，训练样本dataSet，标识向量labels
    dataSetSize = dataSet.shape[0]                   #利用欧氏距离公式，计算两个向量点的距离
    diffMat = np.tile(inX,(dataSetSize,1))-dataSet   #用于分类的向量点减去样本向量点，其中tile函数将inX扩展到与样本相同行数
    sqDiffMat = diffMat**2                           #将减得的值全部平方
    sqDistances = sqDiffMat.sum(axis=1)              #将平方后的值全部相加
    distances = sqDistances**0.5                     #再将相加后的值开根号，便得到了两个向量点的距离。
    sortedDisIndicies = distances.argsort()          #将得到的距离集排序
    classCount = {}                                   
    for i in range(k):                               
        voteIlabel = labels[sortedDisIndicies[i]]    #得到已经排过序的前k个距离所在的label，记录并+1
        classCount[voteIlabel] = classCount.get(voteIlabel,0)+1
    sortedClassCount = sorted(classCount.items(),\
                              key = operator.itemgetter(1),\
                                reverse = True)      #根据对象第一个域的值即出现次数排序，reverse=True降序
    return sortedClassCount[0][0]                    #返回出现次数最大的一个
```
其中计算距离时的公式和高中数学计算两个点的距离类似
现在就可以提供测试的向量，样本数据dataSet，以及标签向量来得到测试结果是否与测试向量原本标识一致
也可以计算此算法的正确率或者错误率