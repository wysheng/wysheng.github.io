---
layout: post
title:  "基于DNN的情感分析模型"
date:   2018-3-18 22:55:16
categories: 深度学习
tags:   DNN 情感分析
---

* content
{:toc}
这里我是直接使用Anaconda 进行环境搭建，使用Ipython Notebook 进行编码和心得的记录，很方便！！！！！

这里的寻来数据来自：
```
train_url = 'https://storage.googleapis.com/mledu-datasets/sparse-data-embedding/train.tfrecord'
train_path = tf.keras.utils.get_file(train_url.split('/')[-1], train_url)
test_url = 'https://storage.googleapis.com/mledu-datasets/sparse-data-embedding/test.tfrecord'
test_path = tf.keras.utils.get_file(test_url.split('/')[-1], test_url)

```
术语表链接：https://storage.googleapis.com/mledu-datasets/sparse-data-embedding/terms.txt 
之前, 先导入各种库：
```
import collections
import math
 
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf
from IPython import display
from sklearn import metrics
 
tf.logging.set_verbosity(tf.logging.ERROR)

```
下面是基本的模型构建和训练验证，里面有详细说明：
```
import sys
from importlib import reload
reload(sys)
# sys.setdefaultencoding("utf-8")
# 这里好像没什么用！！暂时改为二进制方式读取
 
informative_terms = None
with open("terms.txt", "rb") as f:
    informative_terms = list(set(f.read().split()))
print (informative_terms[:20])
 
# 下面是构造嵌入列
# 第一步是构造分类了的特征列
# 第二步 生成嵌入特征列
terms_features_column = tf.feature_column.categorical_column_with_vocabulary_list(key="terms", vocabulary_list=informative_terms)
terms_embedding_column = tf.feature_column.embedding_column(terms_features_column, dimension=2)
 
# 这里只是使用了这一个特征列， 没有其他特征列，所以就是只有一个元素
# 如果需要其他类型的特征列的话，可以直接加入
 
feature_columns = [terms_embedding_column]
 
# 下面是构建优化器
# 第一步是构建一个学习速率是0.1 的优化器，学习速率也就是每次的步长
# 第二步是对对梯度进行规约，防止梯度爆炸
my_optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)
my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)
 
# 接下来就是构建训练模型了，这里是构建的DNN分类器模型
# estimator 又称为估计量，评价者，这里包含了很多的类型的估计量可供使用, 
# DNN 模型的一些重要参数有
#  特征列，隐藏层，优化器，配置等等参数，如需用到可查阅相关api
classifier = tf.estimator.DNNClassifier(
    feature_columns=feature_columns,
    hidden_units=[20,20],
    optimizer=my_optimizer
)
 
# 然后，有了训练模型之后，就该训练这个模型了
# 关于训练模型可评价模型，或者是根据训练好的魔心进行推理，都离不开输入函数
# 这里是先不对输入函数进行讨论，下一篇中会有一些介绍
# 对模型的训练很简单，只需要调用api 的train 函数就行
# 一般常用的参数有input_fn 也就是输入函数，然后是输入对应的总的步数，
# 这里，总的步数对训练出来的模型也有一定的影响
 
classifier.train(
    input_fn=lambda:_input_fn([train_path]),
    steps=1000
)
 
# 在对模型训练完成之后，一般的需要对模型进行评估和测试
# 这里是需要使用训练数据和评估数据分别对模型进行评估
# 然后需要使用测试数据截，对模型进行最终测试，以此检验最终效果
# 这里为什么要把数据集分为评估数据集合测试数据集呢？
# 原因是，评估数据集来源于训练数据分割之后的，经过多次训练，可能发生过拟合的情况
# 然后再添加一个测试数据集，这个测试数据集就是更加普遍的数据
# 用它对模型进行泛化的检验可以很好的估计出 模型是否过拟合与验证数据集
# 这里的验证数据截呢 就是为了方式模型 过拟合与训练数据集
# 这样 经过两层的检验，可以更好的得出优质的模型，也能更好的得出模型的泛化能力
 
# 评估和训练一样也需要输入函数
evaluation_metrics = classifier.evaluate(input_fn=lambda:_input_fn([train_path]), steps=1000)
print ("Training set metrics")
for item in evaluation_metrics:
    print (item, evaluation_metrics[item])
print ("-" * 20)
 
# 同样，对验证数据集进行验证
evaluation_metrics = classifier.evaluate(input_fn=lambda:_input_fn([test_path]), steps=1000)
print ("Test(validate) set metrics")
for item in evaluation_metrics:
    print (item, evaluation_metrics[item])
print ("-" * 20)
 
# 步数增加，产生了过拟合的情况！，建议是1000 就行了

```