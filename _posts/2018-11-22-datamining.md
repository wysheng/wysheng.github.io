---
layout: post
title:  "文本数据挖掘-----词向量"
date:   2018-11-22 17:45:38
categories: 数据挖掘
tags:  文本挖掘 数据挖掘 中文分词
---

* content
{:toc}
中文数据挖掘的难点在于如何把文本变成计算机处理的向量，一个好的词向量方法可以提升分类或者其他应用效果。我把自己接触的词向量技术总结一下，方便自己复习和其他感兴趣的小伙伴交流学习。

使用结巴或者其他中文分词工具分完词后就需要生成词向量了，方便后期的数据挖掘工作的展开。
<font color="red">  词向量技术：我接触的大致可以分成（1）基于统计的方法（2）基于图的方法（3）基于主题模型的方法（4）基于深度学习的方法 </font>   
（1）基于统计的方法：

  相似度，卡方，互信息(优点：可以得到对结果影响大的词；缺点：计算量比较大，需要先验知识，比如类别)

          tf-idf (优点：简单、效果不错，可以得到每个词的权重；缺点：没考虑词的顺序，需要多篇语料才能得到比较好的词)

           n-gram (优点：2-gram以上考虑了词顺序，提升了效果；缺点：随着n的增大，字典迅速扩大，而且训练用的向量特别稀疏)

          bag of words （one hot编码，优点：简单；缺点：没有对词进行过滤，导致词比较多，进而影响字典的数量，而且没有考虑词频，以及词的顺序）

         （2）基于图的方法：

 textrank(优点：把网页排名的算法pagerrank进行变化，得到每个词的重要性，可以针对一篇文章得到重要的词语；缺点：计算复杂度比较高)

          (3)基于主题模型的方法

         LDA (使用了共现矩阵；缺点：没有考虑词序)

PLSA

SVD 

(4)基于深度学习的方法：

word2vec(优点：考虑了词的上下文信息，通过神经网络的投影层得到词向量，属于有监督的学习方法，这里的有监督的意思是把中间词one-hot的编码看成已知的向量进行训练模型，如果考虑是否使用了文章的类别，是无监督的方法；缺点：计算量比较大，训练时间比较久）

doc2vec()

fasttext(优点：优化了word2vec，使速度大范围提升，不用生成词向量了，直接用于分类等其他任务，属于有监督的学习方法)



实际应用中，可能是多种方法联合使用，比如先用tf-idf进行一遍过滤，然后再使用其他方法处理。

先总结这么多，后面有新的东西再加

