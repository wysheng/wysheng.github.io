<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title>用神经网络训练一个文本分类器</title>
    <meta name="description" content="理解聊天机器人的工作原理是非常重要的。聊天机器人内部一个非常重要的组件就是文本分类器。我们看一下文本分类器的神经网络（ANN）的内部工作原理。我们将会使用2层网络（1个隐层）和一个“词包”的方法来组织我们的训练数据。文本分类有3个特点：模式匹配、算法、神经网络。虽然使用多项朴素贝叶斯算法的方法非常有效，但是它有3...">

    <link rel="shortcut icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="icon" href="/favicon.ico?" type="image/x-icon">
    <link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://at.alicdn.com/t/font_8v3czwksspqlg14i.css">
    <link rel="stylesheet" href="/css/main.css ">
    <link rel="canonical" href="http://localhost:4000/2018/05/18/dl/">
    <link rel="alternate" type="application/rss+xml" title="王者勇胜" href="http://localhost:4000/feed.xml ">


    <script>
    // 百度统计代码
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "//hm.baidu.com/hm.js?cf8506e0ef223e57ff6239944e5d46a4";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
    </script>


    <script>
    // google analytics
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-72449510-4', 'auto');
      ga('send', 'pageview');

    </script>



</head>


  <body>

    <header id="top">
    <div class="wrapper">
        <a href="/" class="brand">王者勇胜</a>
        <small>生活不止眼前的代码和苟且，还有诗和远方！</small>
        <button id="headerMenu" class="menu"><i class="fa fa-bars"></i></button>
        <nav id="headerNav">
            <ul>
                <li>
                    
                    <a href="/">
                    
                        <i class="fa fa-home"></i>主页
                    </a>
                </li>

                
                    
                    <li>
                        
                        <a href="/archive/">
                        
                            <i class="fa fa-archive"></i>归档
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/category/">
                        
                            <i class="fa fa-th-list"></i>目录
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/tag/">
                        
                            <i class="fa fa-tags"></i>标签
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/collection/">
                        
                            <i class="fa fa-bookmark"></i>收藏
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/life/">
                        
                            <i class="fa fa-heart"></i>我的诗词
                        </a>
                    </li>
                    
                
                    
                    <li>
                        
                        <a href="/about/">
                        
                            <i class="fa fa-heart"></i>关于我
                        </a>
                    </li>
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
                    
                
            </ul>
        </nav>
    </div>
</header>


        <div class="page clearfix" post>
    <div class="left">
        <h1>用神经网络训练一个文本分类器</h1>
        <div class="label">

            <div class="label-card">
                <i class="fa fa-calendar"></i>2018-05-18
            </div>

            <div class="label-card">
                
            </div>

            <div class="label-card">
                
            </div>

            <div class="label-card">
            


<!-- <span class="point">•</span> -->
<span class="categories">
  <i class="fa fa-th-list"></i>
  
    
        <a href="/category/#深度学习" title="Category: 深度学习" rel="category">深度学习</a>
    
  

  <!-- <span class="point">•</span> -->
</span>


            </div>

            <div class="label-card">
            
<!-- <span class="point">•</span> -->
<span class="pageTag">
  <i class="fa fa-tags"></i>
  
    
        <!--a href="/tag/#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0" title="Tag: 深度学习" rel="tag">深度学习</a-->
        <a href="/tag/#深度学习" title="Tag: 深度学习" rel="tag">深度学习</a>&nbsp;
    
        <!--a href="/tag/#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" title="Tag: 神经网络" rel="tag">神经网络</a-->
        <a href="/tag/#神经网络" title="Tag: 神经网络" rel="tag">神经网络</a>
    
  

</span>

            </div>

        </div>
        <hr>
        <article itemscope itemtype="http://schema.org/BlogPosting">
        
<p>理解聊天机器人的工作原理是非常重要的。聊天机器人内部一个非常重要的组件就是文本分类器。我们看一下文本分类器的神经网络（ANN）的内部工作原理。
<img src="http://jbcdn2.b0.upaiyun.com/2017/08/3f0fc87336415b5dec8a04dc523db2ac.png" alt="" />
我们将会使用2层网络（1个隐层）和一个“词包”的方法来组织我们的训练数据。文本分类有3个特点：模式匹配、算法、神经网络。虽然使用多项朴素贝叶斯算法的方法非常有效，但是它有3个致命的缺陷：</p>

<p>(1)这个算法输出一个分数而不是一个概率。我们可以使用概率来忽略特定阈值以下的预测结果。这类似于忽略收音机中的噪声。
(2)这个算法从一个样本中学习一个分类中包含什么，而不是一个分类中不包含什么。一个分类中不包含什么的的学习模式往往也很重要。
(3)不成比例的大训练集的分类将会导致扭曲的分类分数，迫使算法相对于分类规模来调整输出分数，这并不理想。
和它“天真”的对手一样，这种分类器并不试图去理解句子的含义，而仅仅对它进行分类。事实上，所谓的“人工智能聊天机器人”并不理解语言，但那是另一个故事。
如果你刚接触人工神经网络，这是它的工作原理。
理解分类算法，请看这里。
我们来逐个分析文本分类器的每个部分。我们将按照以下顺序：</p>

<p>1.引用需要的库
2.提供训练集
3.整理数据
4.迭代：编写代码+测试预测结果+调整模型
5.抽象
代码在这里，我们使用ipython notebook这个在数据科学项目上非常高效的工具。代码语法是python。
我们首先导入自然语言工具包。我们需要一个可靠的方法将句子切分成词并且将单词词干化处理。</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># use natural language toolkit
import nltk
from nltk.stem.lancaster import LancasterStemmer
import os
import json
import datetime
stemmer = LancasterStemmer()
</code></pre></div></div>
<p>下面是我们的训练集，12个句子属于3个类别（“意图”）。</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># 3 classes of training data
training_data = []
training_data.append({"class":"greeting", "sentence":"how are you?"})
training_data.append({"class":"greeting", "sentence":"how is your day?"})
training_data.append({"class":"greeting", "sentence":"good day"})
training_data.append({"class":"greeting", "sentence":"how is it going today?"})
 
training_data.append({"class":"goodbye", "sentence":"have a nice day"})
training_data.append({"class":"goodbye", "sentence":"see you later"})
training_data.append({"class":"goodbye", "sentence":"have a nice day"})
training_data.append({"class":"goodbye", "sentence":"talk to you soon"})
 
training_data.append({"class":"sandwich", "sentence":"make me a sandwich"})
training_data.append({"class":"sandwich", "sentence":"can you make a sandwich?"})
training_data.append({"class":"sandwich", "sentence":"having a sandwich today?"})
training_data.append({"class":"sandwich", "sentence":"what's for lunch?"})
print ("%s sentences in training data" % len(training_data))

</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>12 sentences in training data
</code></pre></div></div>
<p>现在我们可以将数据结构组织为：documents, classes 和words.</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>words = []
classes = []
documents = []
ignore_words = ['?']
# loop through each sentence in our training data
for pattern in training_data:
    # tokenize each word in the sentence
    w = nltk.word_tokenize(pattern['sentence'])
    # add to our words list
    words.extend(w)
    # add to documents in our corpus
    documents.append((w, pattern['class']))
    # add to our classes list
    if pattern['class'] not in classes:
        classes.append(pattern['class'])
 
# stem and lower each word and remove duplicates
words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]
words = list(set(words))
 
# remove duplicates
classes = list(set(classes))
 
print (len(documents), "documents")
print (len(classes), "classes", classes)
print (len(words), "unique stemmed words", words)
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>12 documents
3 classes ['greeting', 'goodbye', 'sandwich']
26 unique stemmed words ['sandwich', 'hav', 'a', 'how', 'for', 'ar', 'good', 'mak', 'me', 'it', 'day', 'soon', 'nic', 'lat', 'going', 'you', 'today', 'can', 'lunch', 'is', "'s", 'see', 'to', 'talk', 'yo', 'what']
</code></pre></div></div>
<p>注意每个单词都是词根并且小写。词根有助于机器将“have”和“having”等同起来。同时我们也不关心大小写。
<img src="http://jbcdn2.b0.upaiyun.com/2017/08/339dcac37e068d2783eff6ec4e21eb24.png" alt="" />
我们将训练集中的每个句子转换为词包。</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># create our training data
training = []
output = []
# create an empty array for our output
output_empty = [0] * len(classes)

# training set, bag of words for each sentence
for doc in documents:
    # initialize our bag of words
    bag = []
    # list of tokenized words for the pattern
    pattern_words = doc[0]
    # stem each word
    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]
    # create our bag of words array
    for w in words:
        bag.append(1) if w in pattern_words else bag.append(0)

    training.append(bag)
    # output is a '0' for each tag and '1' for current tag
    output_row = list(output_empty)
    output_row[classes.index(doc[1])] = 1
    output.append(output_row)

# sample training/output
i = 0
w = documents[i][0]
print ([stemmer.stem(word.lower()) for word in w])
print (training[i])
print (output[i])
</code></pre></div></div>
<p>’’’
[‘how’, ‘ar’, ‘you’, ‘?’]
[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[1, 0, 0]
‘’’
上面的步骤是文本分类中的一个经典步骤：每个训练句子被转化为一个包含0和1的数组，而不是语料库中包含独特单词的数组。
[‘how’, ‘are’, ‘you’, ‘?’]
被词干化为：
[[‘how’, ‘ar’, ‘you’, ‘?’]
然后转换为输入词包的形式：1代表单词存在于词包中（忽略问号？）
[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
输出：第一类
[1, 0, 0]
注意：一个句子可以有多个分类，也可以没有。确保理解上面的内容，仔细阅读代码直到你理解它。
机器学习的第一步是要有干净的数据
<img src="http://jbcdn2.b0.upaiyun.com/2017/08/0f3abe8c32b873ad8ffa8c728a59102d.png" alt="" />
接下来我们的学习2层神经网络的核心功能。
如果你是人工神经网络新手，这里是它的工作原理
我们使用numpy，原因是它可以提供快速的矩阵乘法运算。
<img src="http://jbcdn2.b0.upaiyun.com/2017/08/f397d71e69f5ebeaf6a3bf0522861d69.png" alt="" />
我们使用sigmoid函数对值进行归一化，用其导数来衡量错误率。通过不断迭代和调整，直到错误率低到一个可以接受的值。</p>

<p>下面我们也实现了bag-of-words函数，将输入的一个句子转化为一个包含0和1的数组。这就是转换训练数据，得到正确的转换数据至关重要。</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import numpy as np
import time

# compute sigmoid nonlinearity
def sigmoid(x):
    output = 1/(1+np.exp(-x))
    return output

# convert output of sigmoid function to its derivative
def sigmoid_output_to_derivative(output):
    return output*(1-output)

def clean_up_sentence(sentence):
    # tokenize the pattern
    sentence_words = nltk.word_tokenize(sentence)
    # stem each word
    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]
    return sentence_words

# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence
def bow(sentence, words, show_details=False):
    # tokenize the pattern
    sentence_words = clean_up_sentence(sentence)
    # bag of words
    bag = [0]*len(words)  
    for s in sentence_words:
        for i,w in enumerate(words):
            if w == s: 
                bag[i] = 1
                if show_details:
                    print ("found in bag: %s" % w)

    return(np.array(bag))

def think(sentence, show_details=False):
    x = bow(sentence.lower(), words, show_details)
    if show_details:
        print ("sentence:", sentence, "n bow:", x)
    # input layer is our bag of words
    l0 = x
    # matrix multiplication of input and hidden layer
    l1 = sigmoid(np.dot(l0, synapse_0))
    # output layer
    l2 = sigmoid(np.dot(l1, synapse_1))
    return l2
</code></pre></div></div>
<p>现在我们对神经网络训练函数进行编码，创造连接权重。别太激动，这主要是矩阵乘法——来自中学数学课堂。
<img src="http://jbcdn2.b0.upaiyun.com/2017/02/e0e648def4862d31563d98fa1d9e60a4.jpg" alt="" />
我们现在准备去构建我们的神经网络模型，我们将连接权重保存为json文件。</p>

<p>你应该尝试不同的“α”（梯度下降参数），看看它是如何影响错误率。此参数有助于错误调整，并找到最低错误率：</p>

<p>synapse_0 += alpha * synapse_0_weight_update
<img src="http://jbcdn2.b0.upaiyun.com/2017/08/41e84709aaf122cd41527923403b8e99.png" alt="" />
我们在隐藏层使用了20个神经元，你可以很容易地调整。这些参数将随着于您的训练数据规模的不同而不同，将错误率调整到低于10 ^ – 3是比较合理的。</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>X = np.array(training)
y = np.array(output)

start_time = time.time()

train(X, y, hidden_neurons=20, alpha=0.1, epochs=100000, dropout=False, dropout_percent=0.2)

elapsed_time = time.time() - start_time
print ("processing time:", elapsed_time, "seconds")
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Training with 20 neurons, alpha:0.1, dropout:False 
Input matrix: 12x26    Output matrix: 1x3
delta after 10000 iterations:0.0062613597435
delta after 20000 iterations:0.00428296074919
delta after 30000 iterations:0.00343930779307
delta after 40000 iterations:0.00294648034566
delta after 50000 iterations:0.00261467859609
delta after 60000 iterations:0.00237219554105
delta after 70000 iterations:0.00218521899378
delta after 80000 iterations:0.00203547284581
delta after 90000 iterations:0.00191211022401
delta after 100000 iterations:0.00180823798397
saved synapses to: synapses.json
processing time: 6.501226902008057 seconds
</code></pre></div></div>
<p>synapse.json文件中包含了全部的连接权重，这就是我们的模型。
一旦连接权重已经计算完成，对于分类来说只需要classify()函数了：大约15行代码</p>

<p>备注：如果训练集有变化，我们的模型需要重新计算。对于非常大的数据集，这需要较长的时间。</p>

<p>现在我们可以生成一个句子属于一个或者多个分类的概率了。它的速度非常快，这是因为我们之前定义的think()函数中的点积运算。</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># probability threshold
ERROR_THRESHOLD = 0.2
# load our calculated synapse values
synapse_file = 'synapses.json' 
with open(synapse_file) as data_file: 
    synapse = json.load(data_file) 
    synapse_0 = np.asarray(synapse['synapse0']) 
    synapse_1 = np.asarray(synapse['synapse1'])

def classify(sentence, show_details=False):
    results = think(sentence, show_details)

    results = [[i,r] for i,r in enumerate(results) if r&gt;ERROR_THRESHOLD ] 
    results.sort(key=lambda x: x[1], reverse=True) 
    return_results =[[classes[r[0]],r[1]] for r in results]
    print ("%s n classification: %s" % (sentence, return_results))
    return return_results

classify("sudo make me a sandwich")
classify("how are you today?")
classify("talk to you tomorrow")
classify("who are you?")
classify("make me some lunch")
classify("how was your lunch today?")
print()
classify("good day", show_details=True)
</code></pre></div></div>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;strong&gt;sudo make me a sandwich &lt;/strong&gt;
 [['sandwich', 0.99917711814437993]]
&lt;strong&gt;how are you today? &lt;/strong&gt;
 [['greeting', 0.99864563257858363]]
&lt;strong&gt;talk to you tomorrow &lt;/strong&gt;
 [['goodbye', 0.95647479275905511]]
&lt;strong&gt;who are you? &lt;/strong&gt;
 [['greeting', 0.8964283843977312]]
&lt;strong&gt;make me some lunch&lt;/strong&gt; 
 [['sandwich', 0.95371924052636048]]
&lt;strong&gt;how was your lunch today? &lt;/strong&gt;
 [['greeting', 0.99120883810944971], ['sandwich', 0.31626066870883057]]
</code></pre></div></div>
<p>你可以用其它语句、不同概率来试验几次，也可以添加训练数据来改进／扩展当前的模型。尤其注意用很少的训练数据就得到稳定的预测结果。</p>

<p>有一些句子将会产生多个预测结果（高于阈值）。你需要给你的程序设定一个合适的阈值。并非所有的文本分类方案都是相同的：一些预测情况比其他预测需要更高的置信水平。</p>

<p>最后这个分类结果展示了一些内部的细节：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>found in bag: good
found in bag: day
sentence: **good day** 
 bow: [0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
good day 
 [['greeting', 0.99664077655648697]]
</code></pre></div></div>
<p>从这个句子的词包中可以看到，有两个单词和我们的词库是匹配的。同时我们的神经网络从这些 0 代表的非匹配词语中学习了。
如果提供一个仅仅有一个常用单词 ‘a’ 被匹配的句子，那我们会得到一个低概率的分类结果A：</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>found in bag: a
sentence: **a burrito! **
 bow: [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
a burrito! 
 [['sandwich', 0.61776860634647834]]
</code></pre></div></div>
<p>现在你已经掌握了构建聊天机器人的一些基础知识结构，它能处理大量不同的意图，并且对于有限或者海量的训练数据都能很好的适配。想要为某个意图添加一个或者多个响应实在轻而易举，就不必多讲了</p>


        </article>
        <hr>

        
        
            
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                        
                        <h2 id="similar_posts">Similar Posts</h2>
                        <ul>
                        
                        <li class="relatedPost">
                            <a href="/2019/01/19/dl/">深度学习与语音识别—常用声学模型简介
                            
                            </a>
                        </li>
                        
                        
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
            
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
                
                    
                
                    
                
            
        
        
            </ul>
        

        <div class="post-recent">
    <div class="pre">
        
        <p><strong>上一篇</strong> <a href="/2018/03/19/dl2/">基于DNN的情感分析模型</a></p>
        
    </div>
    <div class="nex">

        
        <p><strong>下一篇</strong> <a href="/2018/06/15/datamining/">用Python开发一个迷你打飞机的游戏</a></p>
        
    </div>
</div>


        <h2 id="comments">Comments</h2>
        


<div id="disqus_thread"></div>
<script>
    /**
     * RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
     * LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables
     */

    var disqus_config = function() {
        this.page.url = 'http://localhost:4000/2018/05/18/dl/'; // Replace PAGE_URL with your page's canonical URL variable
        this.page.identifier = 'http://localhost:4000/2018/05/18/dl/'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };

    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document,
            s = d.createElement('script');

        s.src = '//wysheng.disqus.com/embed.js';

        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>




    </div>
    <button class="anchor"><i class="fa fa-anchor"></i></button>
    <div class="right">
        <div class="wrap">

            <!-- Content -->
            <div class="side content">
                <div>
                    Content
                </div>
                <ul id="content-side" class="content-ul">
                    
                    <li><a href="#similar_posts">Similar Posts</a></li>
                    
                    <li><a href="#comments">Comments</a></li>
                </ul>
            </div>
            <!-- 其他div框放到这里 -->
            <!-- <div class="side">bbbb</div> -->
        </div>
    </div>
</div>
<script>
/**
 * target _blank
 */
(function() {
    var aTags = document.querySelectorAll('article a:not([id])')
    for (var i = 0; i < aTags.length; i++) {
        aTags[i].setAttribute('target', '_blank')
    }
}());
</script>
<script src="/js/pageContent.js " charset="utf-8"></script>


    <footer class="site-footer">


    <div class="wrapper">

        <p class="description">
             好好学习，天天向上！ 
        </p>
        <p class="contact">
            Contact me at: 
            <a href="https://github.com/wysheng" title="GitHub"><i class="fa fa-github" aria-hidden="true"></i></a>  
            <a href="mailto:wyshengcn@163.com" title="email"><i class="fa fa-envelope-o" aria-hidden="true"></i></a>        
        </p>
        <p>
            本站总访问量<span id="busuanzi_value_site_pv"></span>次，本站访客数<span id="busuanzi_value_site_uv"></span>人次，本文总阅读量<span id="busuanzi_value_page_pv"></span>次
        </p>
        <p class="power">
            <span>
                Site powered by <a href="https://jekyllrb.com/">Jekyll</a> & <a href="https://pages.github.com/">Github Pages</a>.
            </span>
            <span>
                Theme designed by <a href="https://github.com/Gaohaoyang">HyG</a>.
            </span>
        </p>
    </div>
</footer>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

    <div class="back-to-top">
    <a href="#top" data-scroll>
        <i class="fa fa-arrow-up" aria-hidden="true"></i>
    </a>
</div>

    <script src=" /js/main.js " charset="utf-8"></script>
    <script src=" /js/smooth-scroll.min.js " charset="utf-8"></script>
    <script type="text/javascript">
      smoothScroll.init({
        speed: 500, // Integer. How fast to complete the scroll in milliseconds
        easing: 'easeInOutCubic', // Easing pattern to use
        offset: 20, // Integer. How far to offset the scrolling anchor location in pixels
      });
    </script>
    <!-- <script src=" /js/scroll.min.js " charset="utf-8"></script> -->
  </body>

</html>
