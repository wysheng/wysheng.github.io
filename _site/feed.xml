<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>王者勇胜</title>
    <description></description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 27 Jan 2019 01:48:46 +0800</pubDate>
    <lastBuildDate>Sun, 27 Jan 2019 01:48:46 +0800</lastBuildDate>
    <generator>Jekyll v3.8.5</generator>
    
      <item>
        <title>Hello World!</title>
        <description>
&lt;img src=&quot;https://timgsa.baidu.com/timg?image&amp;amp;quality=80&amp;amp;size=b9999_10000&amp;amp;sec=1548499900740&amp;amp;di=bfd99b433569f2063994beef5364ef04&amp;amp;imgtype=0&amp;amp;src=http%3A%2F%2Fi10.hoopchina.com.cn%2Fhupuapp%2Fbbs%2F128729619991460%2Fthread_128729619991460_20180823182744_s_49705_w_300_h_300_20087.gif&quot; alt=&quot;&quot; /&gt;

</description>
        <pubDate>Fri, 25 Jan 2019 00:00:00 +0800</pubDate>
        <link>http://localhost:4000/2019/01/25/test/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/01/25/test/</guid>
        
        <category>JavaScript</category>
        
        
        <category>JavaScript</category>
        
      </item>
    
      <item>
        <title>深度学习与语音识别—常用声学模型简介</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#1混合声学模型&quot; id=&quot;markdown-toc-1混合声学模型&quot;&gt;（1）混合声学模型&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2端到端的声学模型&quot; id=&quot;markdown-toc-2端到端的声学模型&quot;&gt;（2）端到端的声学模型&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#各个模型的优缺点介绍&quot; id=&quot;markdown-toc-各个模型的优缺点介绍&quot;&gt;各个模型的优缺点介绍&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
006年，Hinton提出深度学习网络，指出深度神经网络因为层数过多导致训练参数多的问题可以利用逐层初始化解决。在工业界和学术界掀起了深度学习的浪潮，并在语音识别和图像处理领域取得了巨大成功。2011年微软和谷歌在语音识别上采用DNN模型，将词错误率降低20%-30%。这里的DNN主要采用的是DBN，即深度置信网络。随着语音识别以及深度学习的发展，研究人员发现将CNN和RNN模型应用于语音识别领域可以取得更好的效果。本文中主要介绍声学模型的优缺点。

    CNN模型，即卷积神经网络，最开始应用于图像处理。而语音识别中的频谱图，应用于CNN模型，可以克服传统语音识别中采用时间、频率而导致的不稳定问题。DBN和CNN模型没有考虑语音之间的关联信息。而RNN模型，充分考虑了语音之间的相互关系，因此取得更加好的效果。现有的最好的基于深度学习的语音识别一般是基于DBN+CNN+RNN模型的。

上述总结成为，现有的声学模型建立，一般可分为：
&lt;h2 id=&quot;1混合声学模型&quot;&gt;（1）混合声学模型&lt;/h2&gt;

混合高斯-隐马尔科夫模型   GMM-HMM

深度神经网络-隐马尔科夫模型   DNN-HMM

深度循环神经网络-隐马尔科夫模型   RNN-HMM

深度卷积神经网络-隐马尔科夫模型   CNN-HMM

&lt;h2 id=&quot;2端到端的声学模型&quot;&gt;（2）端到端的声学模型&lt;/h2&gt;

连接时序分类-长短时记忆模型CTC-LSTM

注意力模型Attention

&lt;h2 id=&quot;各个模型的优缺点介绍&quot;&gt;各个模型的优缺点介绍&lt;/h2&gt;

（1）基于GMM-HMM的声学模型

优点：GMM训练速度快

           声学模型较小，容易移植到嵌入式平台

缺点：GMM没有利用帧的上下文信息

            GMM不能学习深层非线性特征变换

（2）基于DNN-HMM模型

优点： DNN能利用帧的上下文信息，比如前后个扩展5帧

             DNN能学习深层非线性特征变换，表现优于GMM

缺点： 不能利用历史信息来辅助当前任务

（3）基于RNN-HMM模型：

优点：  RNN能有效利用历史信息，将历史消息持久化

              在很多任务上，RNN性能变现优于DNN

缺点：  RNN随着层数的增加，会导致梯度爆炸或者梯度消失

（4）基于CNN-HMM声学模型

优点：CNN对于语音信号，采用时间延迟卷积神经网络可以很好地对信号进行描述学习

            CNN比其他神经网络更能捕捉到特征的不变形
</description>
        <pubDate>Sat, 19 Jan 2019 07:15:26 +0800</pubDate>
        <link>http://localhost:4000/2019/01/19/dl/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/01/19/dl/</guid>
        
        <category>神经网络</category>
        
        
        <category>深度学习</category>
        
      </item>
    
      <item>
        <title>Kmeans文本聚类实施过程</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#1训练词向量&quot; id=&quot;markdown-toc-1训练词向量&quot;&gt;1、训练词向量&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#2对已经准备好的聚类的语料xhj_wordsegmenttxt进行两步操作清理分词&quot; id=&quot;markdown-toc-2对已经准备好的聚类的语料xhj_wordsegmenttxt进行两步操作清理分词&quot;&gt;2、对已经准备好的聚类的语料（XHJ_wordsegment.txt）进行两步操作：清理，分词&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3计算每一行的文本的向量和&quot; id=&quot;markdown-toc-3计算每一行的文本的向量和&quot;&gt;3、计算每一行的文本的向量和&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#4kmeans聚类&quot; id=&quot;markdown-toc-4kmeans聚类&quot;&gt;4、kmeans聚类&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#5抽取聚类结果&quot; id=&quot;markdown-toc-5抽取聚类结果&quot;&gt;5、抽取聚类结果&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;1训练词向量&quot;&gt;1、训练词向量&lt;/h2&gt;
参考资料url：http://www.52nlp.cn/中英文维基百科语料上的word2vec实验
①准备数据，这里假设使用wiki百科的1G数据，其中需要做一个繁体转简体，转格式为utf8，分词过程，参见上面的博客，这里我已经转好了，下载地址见百度网盘：https://pan.baidu.com/s/1htn3gig passwd:d6ss。
②安装好python以及对应的模块 gensim，这里我们是用 gensim训练词向量
③写训练词向量的python脚本，参见上面的博客
④开始训练

开始词向量的训练，如下命令，请在命令行下执行：
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python train_word2vec_model.py wiki.zh.text.jian.utf-8.seg wiki.zh.text.model wiki.zh.text.vector &amp;gt;log.txt &amp;amp;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;2对已经准备好的聚类的语料xhj_wordsegmenttxt进行两步操作清理分词&quot;&gt;2、对已经准备好的聚类的语料（XHJ_wordsegment.txt）进行两步操作：清理，分词&lt;/h2&gt;

&lt;h2 id=&quot;3计算每一行的文本的向量和&quot;&gt;3、计算每一行的文本的向量和&lt;/h2&gt;

(1)由于聚类需要计算每一行文本的向量，这里我们将所有分好的词的向量和作为该行文本的向量表示
(2)由于文本数据过多，所以我们将文本分按照10000条为单位进行了分割，分别求各个文件中文本的向量和，最终再做一次合并
(3)文本分割
(4)DIV()
(5)分别求每个文件中文本的向量和
(6)calVec()
(7)合并求出来的每个文件的向量和
(8)sumvec()

&lt;h2 id=&quot;4kmeans聚类&quot;&gt;4、kmeans聚类&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;save_cluster_results()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;5抽取聚类结果&quot;&gt;5、抽取聚类结果&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;saveRes()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
具体代码参见： 
https://github.com/aiainui/kmeans/blob/master/main.py
</description>
        <pubDate>Wed, 16 Jan 2019 03:45:36 +0800</pubDate>
        <link>http://localhost:4000/2019/01/16/datamining/</link>
        <guid isPermaLink="true">http://localhost:4000/2019/01/16/datamining/</guid>
        
        <category>数据挖掘</category>
        
        <category>kmeans</category>
        
        <category>机器学习</category>
        
        
        <category>数据挖掘</category>
        
      </item>
    
      <item>
        <title>文本数据挖掘-----词向量</title>
        <description>中文数据挖掘的难点在于如何把文本变成计算机处理的向量，一个好的词向量方法可以提升分类或者其他应用效果。我把自己接触的词向量技术总结一下，方便自己复习和其他感兴趣的小伙伴交流学习。

使用结巴或者其他中文分词工具分完词后就需要生成词向量了，方便后期的数据挖掘工作的展开。
&lt;font color=&quot;red&quot;&gt;  词向量技术：我接触的大致可以分成（1）基于统计的方法（2）基于图的方法（3）基于主题模型的方法（4）基于深度学习的方法 &lt;/font&gt;
（1）基于统计的方法：

  相似度，卡方，互信息(优点：可以得到对结果影响大的词；缺点：计算量比较大，需要先验知识，比如类别)

          tf-idf (优点：简单、效果不错，可以得到每个词的权重；缺点：没考虑词的顺序，需要多篇语料才能得到比较好的词)

           n-gram (优点：2-gram以上考虑了词顺序，提升了效果；缺点：随着n的增大，字典迅速扩大，而且训练用的向量特别稀疏)

          bag of words （one hot编码，优点：简单；缺点：没有对词进行过滤，导致词比较多，进而影响字典的数量，而且没有考虑词频，以及词的顺序）

         （2）基于图的方法：

 textrank(优点：把网页排名的算法pagerrank进行变化，得到每个词的重要性，可以针对一篇文章得到重要的词语；缺点：计算复杂度比较高)

          (3)基于主题模型的方法

         LDA (使用了共现矩阵；缺点：没有考虑词序)

PLSA

SVD 

(4)基于深度学习的方法：

word2vec(优点：考虑了词的上下文信息，通过神经网络的投影层得到词向量，属于有监督的学习方法，这里的有监督的意思是把中间词one-hot的编码看成已知的向量进行训练模型，如果考虑是否使用了文章的类别，是无监督的方法；缺点：计算量比较大，训练时间比较久）

doc2vec()

fasttext(优点：优化了word2vec，使速度大范围提升，不用生成词向量了，直接用于分类等其他任务，属于有监督的学习方法)

</description>
        <pubDate>Fri, 23 Nov 2018 01:45:38 +0800</pubDate>
        <link>http://localhost:4000/2018/11/23/datamining/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/11/23/datamining/</guid>
        
        <category>文本挖掘</category>
        
        <category>数据挖掘</category>
        
        <category>中文分词</category>
        
        
        <category>数据挖掘</category>
        
      </item>
    
      <item>
        <title>TF-IDF提取文章关键词算法</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#一tf-idf简介&quot; id=&quot;markdown-toc-一tf-idf简介&quot;&gt;一、TF-IDF简介&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#二算法实现&quot; id=&quot;markdown-toc-二算法实现&quot;&gt;二、算法实现&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#三测试案例&quot; id=&quot;markdown-toc-三测试案例&quot;&gt;三、测试案例&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#python代码实现&quot; id=&quot;markdown-toc-python代码实现&quot;&gt;python代码实现&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;一tf-idf简介&quot;&gt;一、TF-IDF简介&lt;/h2&gt;

TF-IDF（terms frequency-inverse document frequency）是一种用于信息检索与文本挖掘的常用加权技术。TF-IDF是一种统计方法，用来评估一字词对于一篇文章的重要程度。一个词语对一篇文章的重要性主要是依靠它在文件中出现的次数，如果这个词语在这篇文章中的出现次数越高，则表明这个词语对于这篇文章的重要性越高。同时，它还与这个词语在语料库中出现的文章篇数有关，随着出现的篇数越多，则会降低这个词语在这篇文章中的重要性，具体的算法请看下面。

&lt;h2 id=&quot;二算法实现&quot;&gt;二、算法实现&lt;/h2&gt;

1、在实现这个算法之前，我们需要对一篇文章进行分词，在进行中文分词的时候，推荐一个python库，jieba分词，作者将这个项目发布到了GitHub上，是开源的，GitHub地址https://github.com/fxsjy/jieba

2、TF词频的计算

词频（TF）=某个词语在文章中的出现次数

由于我们需要考虑不同的文章，长度不同，我们需要将词频进行归一化处理

词频（TF）=某个词语在文章中的出现次数/文章的总词数 或者 词频（TF）=某个词语在文章中的出现次数/这篇文章出现最多的词的出现次数

3、IDF的计算

逆文档频率（IDF）=log（语料库的文档总数/包含该词的文档数+1），语料库可以自己去网上下载，计算逆文档频率的原因是为了去除哪些经常出现的词语，比如说，“的”、“我们”、“他”等这类的词语，这些词语对于整篇文档重要性不高、但是出现的频率会比较多，就有可能会影响到我们最后的计算结果，如果是经常出现的词语就不能作为我们文章的关键词。

4、计算TF-IDF的值

TF-IDF = 词频（TF）* 逆文档频率（IDF）

5、排序

对文章词语的TF-IDF值进行排序，我们可以选择提取TF-IDF值比较大的词语

6、总结

TF-IDF算法的优点是简单快速，结果比较符合实际情况。但，TF-IDF算法是单纯的以“词频”来衡量一个词的重要性，就显得不够全面，这些词语就不一定能体现出文章的主要思想突出文章的主题。而且，这种算法也无法体现出词语所处的不同位置对于文章的重要性不同，如果想解决这个问题，我们可以采用对于词语所处的不同位置给他们设定不同的权重。

&lt;h2 id=&quot;三测试案例&quot;&gt;三、测试案例&lt;/h2&gt;

下面的例子是使用jieba库，来实现TF-IDF算法的，下面是文章的内容

有很多不同的数学公式可以用来计算tf-idf。
这边的例子以上述的数学公式来计算。
词频（tf）是一词语出现的次数除以该文件的总词语数。
假如一篇文件的总词语数是100个，而词语“母牛”出现了3次，
那么“母牛”一词在该文件中的词频就是3/100=0.03。
一个计算文件频率（DF）的方法是测定有多少份文件出现过“母牛”一词，
然后除以文件集里包含的文件总数。所以，如果“母牛”一词在1,000份文件出现过，
而文件总数是10,000,000份的话，其逆向文件频率就是log（10,000,000 / 1,000）=4。
最后的tf-idf的分数为0.03 * 4=0.12。
&lt;h2 id=&quot;python代码实现&quot;&gt;python代码实现&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import sys
sys.path.append('../')
 
import jieba
import jieba.analyse
from optparse import OptionParser
 
file_name = &quot;../txt/test.txt&quot;
 
content = open(file_name, 'rb').read()
 
#10表示输出的前10个
tags = jieba.analyse.extract_tags(content, topK=10)
 
print(&quot;,&quot;.join(tags))

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
输出结果

000,文件,母牛,词语,tf,词频,100,idf,10,0.03
</description>
        <pubDate>Sun, 28 Oct 2018 01:45:38 +0800</pubDate>
        <link>http://localhost:4000/2018/10/28/datamining/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/10/28/datamining/</guid>
        
        <category>文本挖掘</category>
        
        <category>数据挖掘</category>
        
        <category>中文分词</category>
        
        
        <category>数据挖掘</category>
        
      </item>
    
      <item>
        <title>基于用户的协同过滤推荐算法java实现（UserCF）</title>
        <description>UserCF的核心思想即为根据用户数据模拟向量相似度，我们根据这个相似度，来找出指定用户的相似用户，然后将相似用户买过的而指定用户没有买的东西推荐给指定用户，推荐度的计算也是结合了相似用户与指定用户的相似度累加。注意这里我们默认是用户的隐反馈行为，所以每一个物品的影响因子默认为1。
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;package&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;csu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CFUtils&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
 
&lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;java&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;util&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HashMap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;java&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;util&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;HashSet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;java&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;util&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Iterator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;java&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;util&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;java&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;util&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Entry&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;java&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;util&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Scanner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;java&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;util&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;Set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
 
&lt;span class=&quot;p&quot;&gt;/**&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;基于用户的协同过滤推荐算法实现&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;D&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;author&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Administrator&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;*&lt;/span&gt;
 &lt;span class=&quot;p&quot;&gt;*/&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;UserCF&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
 
	&lt;span class=&quot;k&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;/**&lt;/span&gt;
		 &lt;span class=&quot;p&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;输入用户&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;--&amp;gt;&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;物品条目&lt;/span&gt;  &lt;span class=&quot;err&quot;&gt;一个用户对应多个物品&lt;/span&gt;
		 &lt;span class=&quot;p&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;用户&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ID&lt;/span&gt;	&lt;span class=&quot;err&quot;&gt;物品&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ID&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;集合&lt;/span&gt;
		 &lt;span class=&quot;p&quot;&gt;*&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;
		 &lt;span class=&quot;p&quot;&gt;*&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;
		 &lt;span class=&quot;p&quot;&gt;*&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;
		 &lt;span class=&quot;p&quot;&gt;*&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;D&lt;/span&gt;		&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;
		 &lt;span class=&quot;p&quot;&gt;*/&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;Scanner&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scanner&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Scanner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;nf&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Input the total users number:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;输入用户总量&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scanner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nextInt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[][]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sparseMatrix&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];//&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;建立用户稀疏矩阵，用于用户相似度计算【相似度矩阵】&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;Map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;Integer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;userItemLength&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HashMap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;lt;&amp;gt;();//&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;存储每一个用户对应的不同物品总数&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;eg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;Map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;Set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;itemUserCollection&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HashMap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;lt;&amp;gt;();//&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;建立物品到用户的倒排表&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;Set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;items&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HashSet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;lt;&amp;gt;();//&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;辅助存储物品集合&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;Map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;Integer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;userID&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HashMap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;lt;&amp;gt;();//&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;辅助存储每一个用户的用户&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ID&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;映射&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;Map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;Integer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idUser&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HashMap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;lt;&amp;gt;();//&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;辅助存储每一个&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ID&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;对应的用户映射&lt;/span&gt;
		&lt;span class=&quot;nf&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Input user--items maping infermation:&amp;lt;eg:A a b d&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;scanner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nextLine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;N&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;++){//&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;依次处理&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;N&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;个用户&lt;/span&gt; &lt;span class=&quot;err&quot;&gt;输入数据&lt;/span&gt;  &lt;span class=&quot;err&quot;&gt;以空格间隔&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;user_item&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scanner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nextLine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot; &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;user_item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;userItemLength&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);//&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;userID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);//&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;用户&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ID&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;与稀疏矩阵建立对应关系&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;idUser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;user_item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
			&lt;span class=&quot;p&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;建立物品&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;用户倒排表&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;++){&lt;/span&gt;
				&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contains&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])){//&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;如果已经包含对应的物品&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;用户映射，直接添加对应的用户&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;itemUserCollection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
				&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{//&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;否则创建对应物品&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;用户集合映射&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;itemUserCollection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HashSet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;());//&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;创建物品&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;用户倒排关系&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;itemUserCollection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]);&lt;/span&gt;
				&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
			&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
		&lt;span class=&quot;nf&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;itemUserCollection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;());&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;计算相似度矩阵【稀疏】&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;Set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Entry&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;Set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;entrySet&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;itemUserCollection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;entrySet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;Iterator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Entry&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;Set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iterator&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;entrySet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iterator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;while&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iterator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hasNext&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()){&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;Set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;commonUsers&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iterator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;next&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;getValue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
			&lt;span class=&quot;n&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;user_u&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;commonUsers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;user_v&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;commonUsers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
					&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;equals&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)){&lt;/span&gt;
						&lt;span class=&quot;n&quot;&gt;continue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
					&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;sparseMatrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;userID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;userID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_v&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;//&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;计算用户&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;与用户&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;都有正反馈的物品总数&lt;/span&gt;
				&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
			&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
		&lt;span class=&quot;nf&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;userItemLength&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;());&lt;/span&gt;
		&lt;span class=&quot;nf&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Input the user for recommendation:&amp;lt;eg:A&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;recommendUser&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scanner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nextLine&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
		&lt;span class=&quot;nf&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;userID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recommendUser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;计算用户之间的相似度【余弦相似性】&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;recommendUserId&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;userID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recommendUser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sparseMatrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;++)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
				&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;!= recommendUserId){
&lt;/span&gt;					&lt;span class=&quot;nf&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idUser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recommendUserId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)+&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;--&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idUser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)+&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;相似度:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sparseMatrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recommendUserId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Math&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;userItemLength&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idUser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recommendUserId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;userItemLength&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idUser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))));&lt;/span&gt;
				&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
		
		&lt;span class=&quot;p&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;计算指定用户&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recommendUser&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;的物品推荐度&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){//&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;遍历每一件物品&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;Set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;String&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;users&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;itemUserCollection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);//&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;得到购买当前物品的所有用户集合&lt;/span&gt;
			&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;!users.contains(recommendUser)){//如果被推荐用户没有购买当前物品，则进行推荐度计算
&lt;/span&gt;				&lt;span class=&quot;n&quot;&gt;double&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;itemRecommendDegree&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;users&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
					&lt;span class=&quot;n&quot;&gt;itemRecommendDegree&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sparseMatrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;userID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recommendUser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;userID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Math&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;userItemLength&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recommendUser&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;userItemLength&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));//&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;推荐度计算&lt;/span&gt;
				&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
				&lt;span class=&quot;nf&quot;&gt;System&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;The item &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot; for &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recommendUser&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;'s recommended degree:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;itemRecommendDegree&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
			&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
		&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;scanner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
	&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
 
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Fri, 12 Oct 2018 18:45:38 +0800</pubDate>
        <link>http://localhost:4000/2018/10/12/datamining/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/10/12/datamining/</guid>
        
        <category>数据挖掘</category>
        
        <category>推荐算法</category>
        
        <category>java</category>
        
        
        <category>数据挖掘</category>
        
      </item>
    
      <item>
        <title>Louvain 社团发现算法学习</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#算法介绍&quot; id=&quot;markdown-toc-算法介绍&quot;&gt;算法介绍：&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#模块度增益的计算请继续看下文&quot; id=&quot;markdown-toc-模块度增益的计算请继续看下文&quot;&gt;模块度增益的计算，请继续看下文&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;算法介绍&quot;&gt;算法介绍：&lt;/h2&gt;

Louvain 算法是基于模块度的社区发现算法，该算法在效率和效果上都表现较好，并且能够发现层次性的社区结构，其优化目标是最大化整个社区网络的模块度。

社区网络的模块度（Modularity）是评估一个社区网络划分好坏的度量方法，它的含义是社区内节点的连边数与随机情况下的边数之差，它的取值范围是 (0,1)，其定义如下：
&lt;img src=&quot;https://img-blog.csdn.net/20170414185610724?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXE1NDcyNzY1NDI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center&quot; alt=&quot;&quot; /&gt;
上式中，Aij代表结点i和j之间的边权值（当图不带权时，边权值可以看成1）。 ki代表结点i的領街边的边权和（当图不带权时，即为结点的度数）。
m为图中所有边的边权和。 ci为结点i所在的社团编号。
模块度的公式定义可以做如下的简化：
&lt;img src=&quot;https://img-blog.csdn.net/20170414193226025?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXE1NDcyNzY1NDI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center&quot; alt=&quot;&quot; /&gt;
其中Sigma in表示社区c内的边的权重之和，Sigma tot表示与社区c内的节点相连的边的权重之和。
我们的目标，是要找出各个结点处于哪一个社团，并且让这个划分结构的模块度最大。
Louvain算法的思想很简单：

1）将图中的每个节点看成一个独立的社区，次数社区的数目与节点个数相同；
2）对每个节点i，依次尝试把节点i分配到其每个邻居节点所在的社区，计算分配前与分配后的模块度变化Delta Q，并记录Delta Q最大的那个邻居节点，如果maxDelta Q&amp;gt;0，则把节点i分配Delta Q最大的那个邻居节点所在的社区，否则保持不变；
3）重复2），直到所有节点的所属社区不再变化；
4）对图进行压缩，将所有在同一个社区的节点压缩成一个新节点，社区内节点之间的边的权重转化为新节点的环的权重，社区间的边权重转化为新节点间的边权重；
5）重复1）直到整个图的模块度不再发生变化。
在写代码时，要注意几个要点：

&lt;ul&gt;
  &lt;li&gt;
    第二步，尝试分配结点时，并不是只尝试独立的结点，也可以尝试所在社区的结点数大于1的点，我在看paper时一开始把这个部分看错了，导致算法出问题。
  &lt;/li&gt;
  &lt;li&gt;
    第三步，重复2）的时候，并不是将每个点遍历一次后就对图进行一次重构，而是要不断循环的遍历每个结点，直到一次循环中所有结点所在社区都不更新了，表示当前网络已经稳定，然后才进行图的重构。
  &lt;/li&gt;
  &lt;li&gt;
    &lt;h2 id=&quot;模块度增益的计算请继续看下文&quot;&gt;模块度增益的计算，请继续看下文&lt;/h2&gt;
    过程如下图所示：
&lt;img src=&quot;https://img-blog.csdn.net/20170414195729001?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXE1NDcyNzY1NDI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center&quot; alt=&quot;&quot; /&gt;
可以看出，louvain是一个启发式的贪心算法。我们需要对模块度进行一个启发式的更新。这样的话这个算法会有如下几个问题：
  &lt;/li&gt;
&lt;/ul&gt;

1：尝试将节点i分配到相邻社团时，如果真的移动结点i，重新计算模块度，那么算法的效率很难得到保证

2：在本问题中，贪心算法只能保证局部最优，而不能够保证全局最优

3：将节点i尝试分配至相邻社团时，要依据一个什么样的顺序

……
第一个问题，在该算法的paper中给了我们解答。我们实际上不用真的把节点i加入相邻社团后重新计算模块度，paper中给了我们一个计算把结点i移动至社团c时，模块度的增益公式：
——————— 
&lt;img src=&quot;https://img-blog.csdn.net/20170414194703112?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXE1NDcyNzY1NDI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center&quot; alt=&quot;&quot; /&gt;
其中Sigma in表示起点终点都在社区c内的边的权重之和，Sigma tot表示入射社区c内的边的权重之和，ki代表结点i的带权度数和，m为所有边权和。

但是该增益公式还是过于复杂，仍然会影响算法的时间效率。

但是请注意，我们只是想通过模块增益度来判断一个结点i是否能移动到社团c中，而我们实际上是没有必要真正的去求精确的模块度，只需要知道，当前的这步操作，模块度是否发生了增长。

因此，就有了相对增益公式的出现：

&lt;img src=&quot;https://img-blog.csdn.net/20170414200159913?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvcXE1NDcyNzY1NDI=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center&quot; alt=&quot;&quot; /&gt;
相对增益的值可能大于1，不是真正的模块度增长值，但是它的正负表示了当前的操作是否增加了模块度。用该公式能大大降低算法的时间复杂度。
第二个问题，该算法的确不能够保证全局最优。但是我们该算法的启发式规则很合理，因此我们能够得到一个十分精确的近似结果。
同时，为了校准结果，我们可以以不同的序列多次调用该算法，保留一个模块度最大的最优结果。
第三个问题，在第二个问题中也出现了，就是给某个结点i找寻領接点时，应当以一个什么顺序？递增or随机or其它规则？我想这个问题需要用实验数据去分析。

在paper中也有提到一些能够使结果更精确的序列。我的思路是，如果要尝试多次取其最优，取随机序列应该是比较稳定比较精确的方式。

</description>
        <pubDate>Fri, 05 Oct 2018 23:45:28 +0800</pubDate>
        <link>http://localhost:4000/2018/10/05/datamining/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/10/05/datamining/</guid>
        
        <category>机器学习</category>
        
        <category>推荐算法</category>
        
        <category>数据挖掘</category>
        
        
        <category>机器学习</category>
        
      </item>
    
      <item>
        <title>使用python提取文章关键词</title>
        <description>提取文章关键词，使用TF-IDF 算法，使用的例子是结合jieba分词，使用FreDist，因为TF-IDF算法需要的是一个语料库，当前语料库只有一篇文章，所以TF-IDF算法就退化成计算文章词频的算法了：
需要记录的是FreqDist的成员函数
	plot(n)，绘制出现次数最多的前n项
	tabulate(n)，该方法接受一个数字n作为参数，会以表格的方式打印出现次数最多的前n项
	most_common(n)，该方法接受一个数字n作为参数，返回出现次数最多的前n项列表
	hapaxes()，返回一个低频项列表
	max()，该方法会返回出现次数最多的项。

代码如下：
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# -*- coding: utf-8 -*-
import requests
from bs4 import BeautifulSoup
import jieba
import re
from nltk.book import *
from pylab import *
from jieba.analyse import *
def stop_words():
	stop_word_list = []
	f = open('stopwords.txt', 'rU',encoding='UTF-8')
	for word in f:
		stop_word_list.append(word.strip())
	return stop_word_list	
r = requests.get('https://blog.csdn.net/chszs/article/details/806585xx802')
soup = BeautifulSoup(r.text, 'lxml')
# 获得主要内容
context = soup.find('article').get_text()
# 进行结巴中文分词，获得字符串数组
jieba.load_userdict('user_dict.txt')
word_list = jieba.cut(context)
word_list_str = (&quot;,&quot;.join(word_list))
word_list = re.split(&quot;,&quot;, word_list_str)
#去掉长度为1的单词，同时去掉停止词
stop_word_list = stop_words()
word_list = [w for w in word_list if (len(w)&amp;gt;1 and (w not in stop_word_list))]
word_freq_list = FreqDist(word_list)
# 根据次品得到前20 项
word_commons = word_freq_list.most_common(20)
for word in word_commons:
	print(word[0], word_freq_list.freq(word[0]))

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
所以下一篇文章中主要解决两个问题，第一个是语料库的问题，我们可以将这个用户的所有文章爬下来，获得该用户的所有文章，然后进行计算，使用TF-IDF算法也就是在当前文章中出现次数最多，在其他文章出现次数越少的越可以代表当前文章，

PS:  

&lt;ol&gt;
  &lt;li&gt;
    如果发现jieba分词的结果不是很准确的时候，可以通过加载用户自定义词典进行修正 jieba.load_userdict(‘xxx.txt’)，需要注意的是该词典文件应该是utf-8编码的
  &lt;/li&gt;
  &lt;li&gt;
    因为这里文本清理使用的是bs4的Beautiful4soup，因为我爬取的是csdn的博客，该博客的主要内容在article标签当中，所以只取article的get_text()即可
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 26 Sep 2018 01:45:38 +0800</pubDate>
        <link>http://localhost:4000/2018/09/26/datamining/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/09/26/datamining/</guid>
        
        <category>文本挖掘</category>
        
        <category>数据挖掘</category>
        
        <category>python</category>
        
        
        <category>数据挖掘</category>
        
      </item>
    
      <item>
        <title>基于内容的推荐 java实现</title>
        <description>这是本人在cousera上学习机器学习的笔记，不能保证其正确性，谨慎参考
看完这一课后Content Based Recommendations 后自己用java实现了一下
1、下图是待处理的数据，代码使用数据和下图一样： 
&lt;img src=&quot;https://img-blog.csdn.net/20170309164640433?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHpoNDc2/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&quot; alt=&quot;&quot; /&gt;
2、思路：对每个用户假定其为一个3维向量（在代码中初始化为[1,1,1]的转置，然后采用梯度下降法不断的对这个3维向量的值进行更新），假设更新到最后的向量值为[0，5，0]的转置，然后使用该向量和电影“Cute puppoes of love”的特征向量进行计算，即可得到该电影的预测分为4.95。 
&lt;img src=&quot;https://img-blog.csdn.net/20170309165741276?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHpoNDc2/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&quot; alt=&quot;&quot; /&gt;
3、使用梯度下降法对某个用户的向量进行更新（我在代码中没有考虑正则化这一问题，现在还不懂正则化，后面学会了就附上加了正则化的）： 
&lt;img src=&quot;https://img-blog.csdn.net/20170309170547804?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHpoNDc2/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&quot; alt=&quot;&quot; /&gt;
下图为没有使用正则化的函数：
&lt;img src=&quot;https://img-blog.csdn.net/20170309170756480?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHpoNDc2/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&quot; alt=&quot;&quot; /&gt;

4、下面仅针对用户carol进行了代码实现
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;public class ContentBase {
    private static int[][] rate_set = { { 5, 5, 0, 0 }, { 5, -1, -1, 0 },
            { -1, 4, 0, -1 }, { 0, 0, 5, 4 }, { 0, 0, 5, -1 } };
    private static double[][] m_feature = { { 0.9, 0 }, { 1.0, 0.01 },
            { 0.99, 0 }, { 0.1, 1.0 }, { 0, 0.9 } };
    //仅针对用户carol进行了代码实现
    public static void main(String[] args) {
        double t = 0.1;
        double[] para = { 1.0, 1.0, 1.0 };
        double[] partial = new double[3];
        double min = 0.0;

        int i = 0, j, u,times=0;
        double temp,temp2;
        //100为用户2的向量学习次数
        while(times++&amp;lt;100){
            min=0.0;
            i=0;
            //该while循环计算代价函数
            while (i &amp;lt; 5) {
                temp = 0.0;
                if (rate_set[i][2] != -1) {
                    for (u = 0; u &amp;lt; 3; u++) {
                        if (u == 0)
                            temp += para[u];
                        else
                            temp += para[u] * m_feature[i][u - 1];
                    }
                    min += (temp - rate_set[i][2]) * (temp - rate_set[i][2]);
                }
                i++;
            }
            System.out.print(&quot;当用户 carol的向量值为[&quot;);
            for(j=0;j&amp;lt;3;j++)
                if(j!=2)
                    System.out.print(para[j]+&quot;,&quot;);
                else
                    System.out.println(para[j]+&quot;]时，min=&quot;+min);
            System.out.println();

            for (j = 0; j &amp;lt; 3; j++) {
                i = 0;
                partial[j] = 0;
                while (i &amp;lt; 5) {
                    temp = 0.0;temp2=0.0;
                    if (rate_set[i][2] != -1) {
                        for (u = 0; u &amp;lt; 3; u++) {
                            if (u == 0)
                                temp += para[u];
                            else
                                temp += para[u] * m_feature[i][u - 1];
                        }
                        temp2 += temp - rate_set[i][2];
                        if (j != 0)
                            temp2 *= m_feature[i][j - 1];
                        partial[j]+=temp2;
                    }
                    i++;
                }
            }
            //根据求得的偏导数 partial来更新某用户的参数值
            for (j = 0; j &amp;lt; 3; j++) {
                para[j] = para[j] - t * partial[j];

            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
4、运行结果： 
&lt;img src=&quot;https://img-blog.csdn.net/20170309172356566?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvdHpoNDc2/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&quot; alt=&quot;&quot; /&gt;
</description>
        <pubDate>Wed, 11 Jul 2018 19:45:38 +0800</pubDate>
        <link>http://localhost:4000/2018/07/11/datamining/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/07/11/datamining/</guid>
        
        <category>数据挖掘</category>
        
        <category>推荐算法</category>
        
        <category>java</category>
        
        
        <category>数据挖掘</category>
        
      </item>
    
      <item>
        <title>机器学习算法——PCA算法介绍以及Java实现</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#pca算法&quot; id=&quot;markdown-toc-pca算法&quot;&gt;PCA算法&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#一算法概述&quot; id=&quot;markdown-toc-一算法概述&quot;&gt;一、算法概述&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#二算法原理&quot; id=&quot;markdown-toc-二算法原理&quot;&gt;二、算法原理&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#三代码实现&quot; id=&quot;markdown-toc-三代码实现&quot;&gt;三、代码实现&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;pca算法&quot;&gt;PCA算法&lt;/h2&gt;
&lt;h2 id=&quot;一算法概述&quot;&gt;一、算法概述&lt;/h2&gt;
主成分分析（PCA）是多元统计分析中用来分析数据的一种方法，PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量，常用于高维数据的降维。

PCA方法最著名的应用应该是在人脸识别中特征提取及数据维，我们知道输入200*200大小的人脸图像，单单提取它的灰度值作为原始特征，则这个原始特征将达到40000维，这给后面分类器的处理将带来极大的难度。在这种情况下，我们必须对数据进行降维。

降维当然意味着信息的丢失，不过鉴于实际数据本身常常存在的相关性，我们可以想办法在降维的同时将信息的损失尽量降低。

例如某淘宝店铺的数据记录为(日期, 浏览量, 访客数, 下单数, 成交数, 成交金额)，从经验我们可以知道，“浏览量”和“访客数”往往具有较强的相关关系，而“下单数”和“成交数”也具有较强的相关关系。这里我们非正式的使用“相关关系”这个词，可以直观理解为“当某一天这个店铺的浏览量较高（或较低）时，我们应该很大程度上认为这天的访客数也较高（或较低）”。后面的章节中我们会给出相关性的严格数学定义。 
这种情况表明，如果我们删除浏览量或访客数其中一个指标，我们应该期待并不会丢失太多信息。因此我们可以删除一个，以降低机器学习算法的复杂度。所以，我们可以采用PCA进行降纬。
&lt;h2 id=&quot;二算法原理&quot;&gt;二、算法原理&lt;/h2&gt;
1、数据准备 
假设有M个样本，每个样本有N个特征，例如第i个（i=1,2,…,M）样本为： 
&lt;img src=&quot;https://img-blog.csdn.net/20180207110748376?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1hpYW9fWWFuZzc3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&quot; alt=&quot;&quot; /&gt;

则M个样本构成了M行N列的数值矩阵A。

2、数据归一化处理 
通常做法是将每一维的数据都减去该维的均值，使每一维的均值都为0。

3、计算协方差矩阵 
协方差是一种用来度量两个随机变量关系的统计量，其定义为： 
&lt;img src=&quot;https://img-blog.csdn.net/20160403102743766&quot; alt=&quot;&quot; /&gt;

M*N样本的协方差矩阵为： 
&lt;img src=&quot;https://img-blog.csdn.net/20180207112736891?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvWGlhb1hpYW9fWWFuZzc3/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast&quot; alt=&quot;&quot; /&gt;

4、求出协方差矩阵的特征值及对应的特征向量 
若AX=λX，则称λ是A的特征值，X是对应的特征向量。实际上可以这样理解：矩阵A作用在它的特征向量X上，仅仅使得X的长度发生了变化，缩放比例就是相应的特征值λ。

特别地，当A是对称矩阵时，A的奇异值等于A的特征值，存在正交矩阵Q（Q-1=QT），使得：

对A进行奇异值分解就能求出所有特征值和Q矩阵。

A∗Q=Q∗DA∗Q=Q∗D,D是由特征值组成的对角矩阵

由特征值和特征向量的定义知，Q的列向量就是A的特征向量。

5、将特征向量按对应的特征值大小从上往下按行排列成矩阵，取前k行组成矩阵P，P为k行n列矩阵

6、Y=AP’ 即为降维到k维后的数据，Y为M行k列矩阵
&lt;h2 id=&quot;三代码实现&quot;&gt;三、代码实现&lt;/h2&gt;
PCA.class
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;mport java.util.ArrayList;
import java.util.Collections;
import java.util.HashMap;
import java.util.Iterator;
import java.util.List;
import java.util.Map;
import java.util.Map.Entry;
import java.util.TreeMap;

import Jama.Matrix;

/*
 * 算法步骤:
 * 1)将原始数据按列组成n行m列矩阵X
 * 2)特征中心化。即每一维的数据都减去该维的均值，使每一维的均值都为0
 * 3)求出协方差矩阵
 * 4)求出协方差矩阵的特征值及对应的特征向量
 * 5)将特征向量按对应的特征值大小从上往下按行排列成矩阵，取前k行组成矩阵p
 * 6)Y=PX 即为降维到k维后的数据
 */
public class PCA {

    private static final double threshold = 0.95;// 特征值阈值

    /**
     * 
     * 使每个样本的均值为0
     * 
     * @param primary
     *            原始二维数组矩阵
     * @return averageArray 中心化后的矩阵
     */
    public double[][] changeAverageToZero(double[][] primary) {
        int n = primary.length;
        int m = primary[0].length;
        double[] sum = new double[m];
        double[] average = new double[m];
        double[][] averageArray = new double[n][m];
        for (int i = 0; i &amp;lt; m; i++) {
            for (int j = 0; j &amp;lt; n; j++) {
                sum[i] += primary[j][i];
            }
            average[i] = sum[i] / n;
        }
        for (int i = 0; i &amp;lt; m; i++) {
            for (int j = 0; j &amp;lt; n; j++) {
                averageArray[j][i] = primary[j][i] - average[i];
            }
        }
        return averageArray;
    }

    /**
     * 
     * 计算协方差矩阵
     * 
     * @param matrix
     *            中心化后的矩阵
     * @return result 协方差矩阵
     */
    public double[][] getVarianceMatrix(double[][] matrix) {
        int n = matrix.length;// 行数
        int m = matrix[0].length;// 列数
        double[][] result = new double[m][m];// 协方差矩阵
        for (int i = 0; i &amp;lt; m; i++) {
            for (int j = 0; j &amp;lt; m; j++) {
                double temp = 0;
                for (int k = 0; k &amp;lt; n; k++) {
                    temp += matrix[k][i] * matrix[k][j];
                }
                result[i][j] = temp / (n - 1);
            }
        }
        return result;
    }

    /**
     * 求特征值矩阵
     * 
     * @param matrix
     *            协方差矩阵
     * @return result 向量的特征值二维数组矩阵
     */
    public double[][] getEigenvalueMatrix(double[][] matrix) {
        Matrix A = new Matrix(matrix);
        // 由特征值组成的对角矩阵,eig()获取特征值
//      A.eig().getD().print(10, 6);
        double[][] result = A.eig().getD().getArray();
        return result;
    }

    /**
     * 标准化矩阵（特征向量矩阵）
     * 
     * @param matrix
     *            特征值矩阵
     * @return result 标准化后的二维数组矩阵
     */
    public double[][] getEigenVectorMatrix(double[][] matrix) {
        Matrix A = new Matrix(matrix);
//      A.eig().getV().print(6, 2);
        double[][] result = A.eig().getV().getArray();
        return result;
    }

    /**
     * 寻找主成分
     * 
     * @param prinmaryArray
     *            原始二维数组数组
     * @param eigenvalue
     *            特征值二维数组
     * @param eigenVectors
     *            特征向量二维数组
     * @return principalMatrix 主成分矩阵
     */
    public Matrix getPrincipalComponent(double[][] primaryArray,
            double[][] eigenvalue, double[][] eigenVectors) {
        Matrix A = new Matrix(eigenVectors);// 定义一个特征向量矩阵
        double[][] tEigenVectors = A.transpose().getArray();// 特征向量转置
        Map&amp;lt;Integer, double[]&amp;gt; principalMap = new HashMap&amp;lt;Integer, double[]&amp;gt;();// key=主成分特征值，value=该特征值对应的特征向量
        TreeMap&amp;lt;Double, double[]&amp;gt; eigenMap = new TreeMap&amp;lt;Double, double[]&amp;gt;(
                Collections.reverseOrder());// key=特征值，value=对应的特征向量；初始化为翻转排序，使map按key值降序排列
        double total = 0;// 存储特征值总和
        int index = 0, n = eigenvalue.length;
        double[] eigenvalueArray = new double[n];// 把特征值矩阵对角线上的元素放到数组eigenvalueArray里
        for (int i = 0; i &amp;lt; n; i++) {
            for (int j = 0; j &amp;lt; n; j++) {
                if (i == j)
                    eigenvalueArray[index] = eigenvalue[i][j];
            }
            index++;
        }

        for (int i = 0; i &amp;lt; tEigenVectors.length; i++) {
            double[] value = new double[tEigenVectors[0].length];
            value = tEigenVectors[i];
            eigenMap.put(eigenvalueArray[i], value);
        }

        // 求特征总和
        for (int i = 0; i &amp;lt; n; i++) {
            total += eigenvalueArray[i];
        }
        // 选出前几个主成分
        double temp = 0;
        int principalComponentNum = 0;// 主成分数
        List&amp;lt;Double&amp;gt; plist = new ArrayList&amp;lt;Double&amp;gt;();// 主成分特征值
        for (double key : eigenMap.keySet()) {
            if (temp / total &amp;lt;= threshold) {
                temp += key;
                plist.add(key);
                principalComponentNum++;
            }
        }
        System.out.println(&quot;\n&quot; + &quot;当前阈值: &quot; + threshold);
        System.out.println(&quot;取得的主成分数: &quot; + principalComponentNum + &quot;\n&quot;);

        // 往主成分map里输入数据
        for (int i = 0; i &amp;lt; plist.size(); i++) {
            if (eigenMap.containsKey(plist.get(i))) {
                principalMap.put(i, eigenMap.get(plist.get(i)));
            }
        }

        // 把map里的值存到二维数组里
        double[][] principalArray = new double[principalMap.size()][];
        Iterator&amp;lt;Entry&amp;lt;Integer, double[]&amp;gt;&amp;gt; it = principalMap.entrySet()
                .iterator();
        for (int i = 0; it.hasNext(); i++) {
            principalArray[i] = it.next().getValue();
        }

        Matrix principalMatrix = new Matrix(principalArray);

        return principalMatrix;
    }

    /**
     * 矩阵相乘
     * 
     * @param primary
     *            原始二维数组
     * 
     * @param matrix
     *            主成分矩阵
     * 
     * @return result 结果矩阵
     */
    public Matrix getResult(double[][] primary, Matrix matrix) {
        Matrix primaryMatrix = new Matrix(primary);
        Matrix result = primaryMatrix.times(matrix.transpose());
        return result;
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
主函数调用PCA：
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import Jama.Matrix;
import java.io.FileWriter;
import java.io.IOException;

public class PCAMain {

    public static void main(String[] args) throws IOException {
        // TODO Auto-generated catch block

        SelectData selectData = new SelectData();
        PCA pca = new PCA();
        //获得样本集
        double[][] primaryArray = selectData.getdatas();
        System.out.println(&quot;--------------------------------------------&quot;);
        double[][] averageArray = pca.changeAverageToZero(primaryArray);
        System.out.println(&quot;--------------------------------------------&quot;);
        System.out.println(&quot;均值0化后的数据: &quot;);
        System.out.println(averageArray.length + &quot;行，&quot;
                + averageArray[0].length + &quot;列&quot;);

        System.out.println(&quot;---------------------------------------------&quot;);
        System.out.println(&quot;协方差矩阵: &quot;);
        double[][] varMatrix = pca.getVarianceMatrix(averageArray);

        System.out.println(&quot;--------------------------------------------&quot;);
        System.out.println(&quot;特征值矩阵: &quot;);
        double[][] eigenvalueMatrix = pca.getEigenvalueMatrix(varMatrix);

        System.out.println(&quot;--------------------------------------------&quot;);
        System.out.println(&quot;特征向量矩阵: &quot;);
        double[][] eigenVectorMatrix = pca.getEigenVectorMatrix(varMatrix);

        System.out.println(&quot;--------------------------------------------&quot;);
        Matrix principalMatrix = pca.getPrincipalComponent(primaryArray, eigenvalueMatrix, eigenVectorMatrix);
        System.out.println(&quot;主成分矩阵: &quot;);
//        principalMatrix.print(6, 3);

        System.out.println(&quot;--------------------------------------------&quot;);
        System.out.println(&quot;降维后的矩阵: &quot;);
        Matrix resultMatrix = pca.getResult(primaryArray, principalMatrix);
//        resultMatrix.print(6, 3);
        int c = resultMatrix.getColumnDimension(); //列数
        int r = resultMatrix.getRowDimension();//行数
        System.out.println(resultMatrix.getRowDimension() + &quot;,&quot; + resultMatrix.getColumnDimension());
    }
}

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Wed, 11 Jul 2018 19:45:38 +0800</pubDate>
        <link>http://localhost:4000/2018/07/11/datamining/</link>
        <guid isPermaLink="true">http://localhost:4000/2018/07/11/datamining/</guid>
        
        <category>特征选择</category>
        
        <category>java</category>
        
        <category>PCA</category>
        
        
        <category>机器学习</category>
        
      </item>
    
  </channel>
</rss>
